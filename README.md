# Designing-and-Deploying-Big-Data-Analytic-framework-using-MapReduce-
The Project “Design and Deployment of Big Data Analytic Framework using MapReduce” is the project that deals with the
concepts of big data applied on real word huge datasets and how machine learning
algorithms (NLP, K-Means, Naïve-Bayes, Deep Learning such as Artificial Neural
Networks, Convolution Neural Networks) can be applied on the results given by Big data
and by torturing that data, it will speak/tells us everything in an appropriate manner
In our project we have use the concepts of Map/Reduce to map and reduce our datasets, we
have 9 datasets and each dataset contains at least 6 Million rows which is sufficient to
make big data concept possible.

In the contemporary world, Data analysis is a challenge. In other words, effective data
analytics helps in analysing the data of any business system. But it is the big data which
helps and kill the process of analysis of data paving way for a success of any business
intelligence system. With the expansion of the industry, the data of the industry also
expands. Then, it is increasingly difficult to handle huge amount of data that gets generated
no matter what’s the business is like, range of fields from social media to finance, flight
data, environment and health. Big Data can be used to assess risk in the insurance industry
and to track reactions to products in real time. Big Data is also used to monitor things as
diverse as wave movements, flight data, traffic data, financial transactions, health and
crime. The challenge of Big Data is how to use it to create something that is value to the
user. How can it be gathered, stored, processed and analysed it to turn the raw data
information to support decision making.
